defaultRules:
  create: true
  rules:
    etcd: false
    kubeScheduler: false

additionalPrometheusRulesMap:
  rule-name:
    groups:
      - name: ${APP}
        rules:
          - alert: AlertmanagerTest
            expr: vector(1)
            labels:
              severity: none
              action: none
            annotations:
              summary: "Test Autofire"
              description: "This fires after 30 seconds automatically for test."
              runbook_url: http://localhost:${HTTPPORT}

          - alert: ${APP}Down
            expr: absent(up{job="amlearn-services"} == 1)
            for: 1m
            labels:
              severity: critical
              action: panic
            annotations:
              summary: "No ${APP} Service Provision"
              description: "The ${APP}-service does not provide an UP signal."
              runbook_url: http://localhost:${HTTPPORT}/service/info

          - alert: ${APP}Pods
            expr: count(${APP}_server_info{job="${APP}-pods"}) > 2
            for: 10s
            labels:
              severity: low
              action: none
            annotations:
              summary: "More than two pods in ${APP}"
              description: "There are more than two ${APP} running! We may have scaled up even more?"
              runbook_url: http://localhost:${HTTPPORT}

          - alert: ${APP}Pods
            expr: count(${APP}_server_info{job="${APP}-pods"}) > 3
            for: 10s
            labels:
              severity: high
              action: none
            annotations:
              summary: "More than three pods in ${APP}"
              description: "There are more than three ${APP} pods running! We may have scaled really hard?"
              runbook_url: http://localhost:${HTTPPORT}

          - alert: ${APP}Pods
            expr: count(${APP}_server_info{job="${APP}-pods"}) > 4
            for: 10s
            labels:
              severity: critical
              action: panic
            annotations:
              summary: "There are {{ $value }} pods in ${APP}"
              description: "There are {{ $value }} ${APP} pods running! We need a bigger machine!"
              runbook_url: http://localhost:${HTTPPORT}

          - alert: ${APP}CPULimit
            expr: sum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{container="${APP}-container"})/sum(kube_pod_container_resource_limits{resource="cpu", container="${APP}-container"}) > 0.1
            for: 1s
            labels:
              severity: low
              action: none
            annotations:
              summary: "10% CPU Limit"
              description: "${APP} pods consume more than 10% of CPU Limit"
              runbook_url: http://localhost:${HTTPPORT}

          - alert: ${APP}CPULimit
            expr: sum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{container="${APP}-container"})/sum(kube_pod_container_resource_limits{resource="cpu", container="${APP}-container"}) > 0.3
            for: 1s
            labels:
              severity: medium
              action: none
            annotations:
              summary: "30% CPU Limit"
              description: "${APP} pods consume more than 30% of CPU Limit"
              runbook_url: http://localhost:${HTTPPORT}

          - alert: ${APP}CPULimit
            expr: sum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{container="${APP}-container"})/sum(kube_pod_container_resource_limits{resource="cpu", container="${APP}-container"}) > 0.8
            for: 1s
            labels:
              severity: high
              action: none
            annotations:
              summary: "80% CPU Limit"
              description: "${APP} pods consume more than 80% of CPU Limit"
              runbook_url: http://localhost:${HTTPPORT}

          - alert: ${APP}CPURequest
            expr: (sum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{container="${APP}-container"}) / sum(cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests{container="${APP}-container"})) * 100  > 33
            for: 10s
            labels:
              severity: low
              action: none
            annotations:
              summary: "More than 33% CPU Request"
              description: "${APP} pods consume more 33% CPU Request, specifically {{ printf \"%.0f\" $value }}%"
              runbook_url: http://localhost:${HTTPPORT}

alertmanager:
  ingress:
    enabled: true
    hosts:
      - localhost
    path: /alert
    ingressClassName: nginx
  admissionWebhooks:
    enabled: false
    patch:
      enabled: false
  tlsProxy.enabled: false
  alertmanagerSpec:
    routePrefix: /alert
  serviceAccount:
    create: true
    name: ""
  podDisruptionBudget:
    enabled: false
    minAvailable: 1
    maxUnavailable: ""

  config:
    #global:
    #  slack_api_url: ${SLACKWEBHOOK}

    route:

      # everything goes to Slack by default. There are routes to other places when needed. 
      receiver: "slack"
      group_by: ["severity"]
      group_wait: 30s
      group_interval: 2m
      repeat_interval: 12h
      routes:
        - match:
            severity: "none"
          receiver: "null"
        - match:
            alertname: "KubeControllerManagerDown"
          receiver: "null"
        - match:
            alertname: "${APP}Down"
          repeat_interval: 2m
          #receiver: "slack"
        - match:
            severity: "critical"
          repeat_interval: 10m
    inhibit_rules:
      - source_match:
          severity: "medium"
        target_match:
          severity: "low"
        equal:
          - alertname
      - source_match:
          severity: "high"
        target_match_re:
          severity: "(low|medium)"
        equal:
          - alertname
      - source_match:
          severity: "critical"
        target_match_re:
          severity: "(low|medium|high)"
        equal:
          - alertname
    receivers:
      - name: "null" 
      - name: "slack"
        slack_configs:
          - send_resolved: true
            api_url: ${SLACKWEBHOOK}
            title: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] Monitoring Event Notification'
            text: >-
              {{ range .Alerts }}
                *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`
                *Description:* {{ .Annotations.description }}
                *Runbook:* {{ .Annotations.runbook_url }}
                *Graph:* <{{ .GeneratorURL }}|Click to see Graph>, if available. Port ${HTTPPORT} may be missing in the URL.
                *Details:*
                {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
                {{ end }}
              {{ end }}


